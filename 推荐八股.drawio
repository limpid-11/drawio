<mxfile host="app.diagrams.net" agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edg/127.0.0.0" version="24.7.8">
  <diagram name="第 1 页" id="iWVoZDMzAQuaJpYY_TNt">
    <mxGraphModel dx="1921" dy="922" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="0" pageScale="1" pageWidth="827" pageHeight="1169" math="0" shadow="0">
      <root>
        <mxCell id="0" />
        <mxCell id="1" parent="0" />
        <mxCell id="mPcpz4siuyIrg-kaVJwC-1" value="协同过滤&lt;div&gt;1）基于用户的&lt;/div&gt;&lt;div&gt;2）基于物品的&lt;/div&gt;&lt;div&gt;实现方法&lt;/div&gt;&lt;div&gt;利用用户item的共现矩阵学习用户和item的表示&lt;/div&gt;&lt;div&gt;内积可以表示成相似度&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;持久化操作&lt;/div&gt;&lt;div&gt;cache()函数与persist()函数&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;正则化调整输入数据的分布，使网络收敛更稳定&lt;/div&gt;&lt;div&gt;BN指对特征进行归一化&lt;/div&gt;&lt;div&gt;LN指对样本进行归一化&lt;/div&gt;&lt;div&gt;图像中多用BN，NLP中多用LN&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;" style="rounded=0;whiteSpace=wrap;html=1;align=left;verticalAlign=top;spacingLeft=4;" parent="1" vertex="1">
          <mxGeometry x="-40" y="-20" width="510" height="420" as="geometry" />
        </mxCell>
        <mxCell id="Wk424I4RCr_Jomv0QLjG-1" value="&lt;div&gt;信息熵的概念：那些接近确定性的分布（输出几乎可以确定）具有较低的熵，那些接近均匀分布的概率分布具有较高的熵。&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;为啥交叉熵用于分类任务？&lt;/div&gt;&lt;div&gt;https://zhuanlan.zhihu.com/p/35709485&lt;br&gt;&lt;/div&gt;&lt;div&gt;（1）与分类正确率相比，无法细分模型的准确度。&lt;/div&gt;&lt;div&gt;（2）与 MSE相比，由于分类模型的输出通常是sigmod或者softmax，所以反向传播计算得到的梯度，如果是交叉熵的话，等于(c(x)-y)*xi，能做到误差越小，梯度越小，误差越大，梯度越大。相较于MSE，收敛速度更快，收敛更稳定。&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;优化器&lt;/div&gt;&lt;div&gt;https://zhuanlan.zhihu.com/p/261695487&lt;br&gt;&lt;/div&gt;&lt;div&gt;作用是通过对下降方向和下降步长定义，是一种梯度下降的策略的体现。&lt;/div&gt;&lt;div&gt;（1）SCD 随机梯度下降，每次只利用一个样本进行提取下降&lt;/div&gt;&lt;div&gt;（2）SCD 带动量，不仅由当前的方向决定，还由之前的决定（一阶动量是指数移动平均值）&lt;/div&gt;&lt;div&gt;（3）NAG 利于跳出局部最优，但是收敛速度慢&lt;/div&gt;&lt;div&gt;（4）AdaGrad 使用二阶动量使得学习率衰减&lt;/div&gt;&lt;div&gt;（5）AdaDelta 使用窗口的指数移动平均数计算二阶动量&lt;/div&gt;&lt;div&gt;（6）Adam 一阶动量调整下降方向+二阶动量使学习率衰减 减小了鞍点附近的震荡&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;特征重要性筛选&lt;/div&gt;&lt;div&gt;GBDT+LR：做个小case好一些&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;" style="rounded=0;whiteSpace=wrap;html=1;align=left;verticalAlign=top;spacingLeft=4;" parent="1" vertex="1">
          <mxGeometry x="-560" y="-20" width="510" height="420" as="geometry" />
        </mxCell>
        <mxCell id="xFfinqP3kLWbWLSmUtOE-1" value="&lt;div&gt;1.说说AUC指标&lt;/div&gt;&lt;div&gt;反映的是&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;2.如何做的特征筛选&lt;/div&gt;&lt;div&gt;Boruta算法&lt;/div&gt;&lt;div&gt;同样通用的方法还有使用树模型，比如有GBDT+LR的方案，通过树模型得到有效的特征信息&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;3.如何做的embedding&lt;/div&gt;&lt;div&gt;node2vec&lt;/div&gt;&lt;div&gt;当然也有一些其他的方案，如矩阵分解，DSSM&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;4.优化器有了解吗&lt;/div&gt;&lt;div&gt;SGD、AdaGrad(一阶动量)、Adam&lt;/div&gt;&lt;div&gt;AdamW&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;5.损失函数有了解吗&lt;/div&gt;&lt;div&gt;MSE损失函数、交叉熵损失函数&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;6.分类问题为什么用损失熵函数&lt;/div&gt;&lt;div&gt;结论是收敛更快，更稳定。&lt;/div&gt;&lt;div&gt;分类模型一般输出都需要经过sigmod或者softmax层，因此反向传播得到的梯度会与误差有关，误差越大，梯度越大，收敛越快。误差小了，梯度也会变小，收敛会稳定。&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;7.大模型与推荐系统&lt;/div&gt;&lt;div&gt;DSSM最初也是nlp领域用来做语义相似度计算的，后面也成为了推荐领域很好用的模型。&lt;br&gt;&lt;/div&gt;&lt;div&gt;大模型中也有相似/可以借鉴的方法。&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;8.不同模型中的embdding都是怎么来的&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;9.冷启动怎么解决&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;10.point-wise，pair-wise，list-wise&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;11.AB实验的原理，主要指标&lt;/div&gt;&lt;div&gt;设计两个版本，在同一时间用两批组成相似的用户进行测试。测试算法是否表现更好。&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;12.广告和推荐的差异&lt;/div&gt;&lt;div&gt;广告是筛选出对目标广告感兴趣的用户，而推荐是为用户推荐感兴趣的item，方法层面我认为很多东西都是可以迁移的。&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;13.&lt;/div&gt;" style="rounded=0;whiteSpace=wrap;html=1;align=left;verticalAlign=top;spacingLeft=4;" vertex="1" parent="1">
          <mxGeometry x="-560" y="410" width="510" height="660" as="geometry" />
        </mxCell>
      </root>
    </mxGraphModel>
  </diagram>
</mxfile>
